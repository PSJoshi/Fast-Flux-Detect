'''

fluxDetect - A Heuristic Approach to Detecting Fast Flux Service Networks
created by Ashutosh Goel
MITRE Corporation
G024-National Intelligence Security
Summer 2011

required libraries: pycap (requires other libraries), jpype, weka (jar file)

'''
from __future__ import with_statement
import os
import subprocess
import time
import math
import networkx
from urlparse import urlparse
import re
from networkx.algorithms import *
import pycap.capture as pcapture
import pycap
import jpype
from jpype import java

#Read URLs from file and put in respective array (benign or malicious)
def read_file(filename, domaintype):
	reader = open(filename,'r')
	domains = reader.readlines()
	reader.close()
	for domain in domains:
	  domaintype.append(domain.rstrip())
	  
def is_valid_ip(ip):
    """Validates IPv4 addresses.
    """
    pattern = re.compile(r"""
        ^
        (?:
          # Dotted variants:
          (?:
            # Decimal 1-255 (no leading 0's)
            [3-9]\d?|2(?:5[0-5]|[0-4]?\d)?|1\d{0,2}
          |
            0x0*[0-9a-f]{1,2}  # Hexadecimal 0x0 - 0xFF (possible leading 0's)
          |
            0+[1-3]?[0-7]{0,2} # Octal 0 - 0377 (possible leading 0's)
          )
          (?:                  # Repeat 0-3 times, separated by a dot
            \.
            (?:
              [3-9]\d?|2(?:5[0-5]|[0-4]?\d)?|1\d{0,2}
            |
              0x0*[0-9a-f]{1,2}
            |
              0+[1-3]?[0-7]{0,2}
            )
          ){0,3}
        |
          0x0*[0-9a-f]{1,8}    # Hexadecimal notation, 0x0 - 0xffffffff
        |
          0+[0-3]?[0-7]{0,10}  # Octal notation, 0 - 037777777777
        |
          # Decimal notation, 1-4294967295:
          429496729[0-5]|42949672[0-8]\d|4294967[01]\d\d|429496[0-6]\d{3}|
          42949[0-5]\d{4}|4294[0-8]\d{5}|429[0-3]\d{6}|42[0-8]\d{7}|
          4[01]\d{8}|[1-3]\d{0,9}|[4-9]\d{0,8}
        )
        $
    """, re.VERBOSE | re.IGNORECASE)
    return pattern.match(ip) is not None

#Gets Domain of a specified URL
def getDomain(url, tlds):
    urlElements = urlparse(url)[1].split('.')
    # urlElements = ["abcde","co","uk"]

    for i in range(-len(urlElements),0):
        lastIElements = urlElements[i:]
        #    i=-3: ["abcde","co","uk"]
        #    i=-2: ["co","uk"]
        #    i=-1: ["uk"] etc

        candidate = ".".join(lastIElements) # abcde.co.uk, co.uk, uk
        wildcardCandidate = ".".join(["*"]+lastIElements[1:]) # *.co.uk, *.uk, *
        exceptionCandidate = "!"+candidate

        # match tlds: 
        if (exceptionCandidate in tlds):
            return ".".join(urlElements[i:]) 
        if (candidate in tlds or wildcardCandidate in tlds):
            return ".".join(urlElements[i-1:])
            # returns "abcde.co.uk"

    raise ValueError("Domain not in global list of TLDs")
'''
def create_graph():
  #benign = open('benignIPs.txt','r')
  #malicious = open('traditionalIPs.txt','r')
  top_nodes = []
  bottom_nodes = []
  while(True):
    line = benign.readline()
    if (line == '' or line == '\n'):
      break
    line = line.rstrip(':\n')
    while(True):
      IP = benign.readline()
      if(IP =='' or IP == '\n'):
	break
      IP = IP.strip()
      top_nodes.append(line)
      bottom_nodes.append(IP)
   # print 'Done with %s'%line
  while(True):
    line = malicious.readline()
    if (line == '' or line == '\n'):
      break
    line = line.rstrip(':\n')
    while(True):
      IP = malicious.readline()
      if(IP =='' or IP == '\n'):
	break
      IP = IP.strip()
      top_nodes.append(line)
      bottom_nodes.append(IP)
    #print 'Done with %s'%line
  for key in lookups:
    for IP in lookups[key]:
      top_nodes.append(key)
      bottom_nodes.append(IP)
  edges = zip(top_nodes,bottom_nodes)
  G = networkx.Graph(edges)
  return G
'''
#Return the 3 Delay Metrics and Their Respective Standard Deviations
def get_delays(url):
  rd = []
  nd = []
  ad = []
  i = 0
  #Iterate 5 times
  while i<5:
    #tcpdump = subprocess.Popen('tcpdump \'tcp[13] & 2 != 0 \' -i eth0 -c 2 -ttt -n', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    p = pcapture.capture()
    p.filter('tcp[13] & 2 != 0')	#Only capture packets that contain a SYN flag 
    start = time.time()			#Before HTTP GET
    #Issue HTTP GET with a connect time out of 10 seconds and a read timeout of 10 seconds, and wait for process to finish
    wget = subprocess.Popen('wget --connect-timeout=10 --read-timeout=10 -O /dev/null %s'%url,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    wget.wait()				
    end = time.time()			#After HTTP GET
    #readtcp = tcpdump.communicate()
    k = 0
    packet = ''
    while True:				#Look for a packet with a SYN flag only
      packet = p.next()
      if(packet != None):
	if len(packet)>2:
	  if type(packet[2]) == pycap.protocol.tcp:
	    if packet[2].flags == 2:
	      break
	   
    SYN = packet			
    while True:				#Look for a packet with  SYN+ACK flag
      packet = p.next()
      if(packet != None):
	if len(packet)>2:
	  if type(packet[2]) == pycap.protocol.tcp:
	    if packet[2].flags == 18:
	      break
    ACK = packet
    rd.append((end-start))		#RD portion of the Document Fetch Delay
    first = float(SYN[-1])		#SYN Packet time
    last = float(ACK[-1])		#SYN+ACK Packet Time
    SYNACK = (last-first)*1.0/1000	#Network Delay
    nd.append(SYNACK)
    start = time.time()			#Before HTTP HI
    #Issue HTTP HI (dummy) method with a maximum process time of 20 seconds, and wait for the process to finish. The no-keepalive ensures that persistent connection is off
    curl = subprocess.Popen('curl -m 20 --no-keepalive -sLX HI %s -o /dev/null'%url,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    curl.wait()
    end = time.time()			#After HTTP HI
    ad.append((end-start))		#AD portion of the Processing Delay
    i+=1
    print 'done with iteration %d'%i
  delays = []
  dfd = []
  pd = []
  i = 0
  while i<5:
    dfd.append(rd[i]-nd[i])		#DFD = RD - ND
    pd.append(ad[i]-nd[i])		#PD = AD - ND
    i+=1
  delays.append(nd)
  delays.append(dfd)
  delays.append(pd)
  return delays

#Return the Mean and Standard Deviation of an Array
def standev(array):
  num = len(array)
  sum = 0
  i = 0
  while i<num:
    sum+=array[i]
    i+=1
  mean = sum/num
  sq_diff_sum = 0
  i = 0
  while i<num:
    diff = array[i]-mean
    sq_diff_sum += diff*diff
    i+=1
  deviance = sq_diff_sum/num
  sd = math.sqrt(deviance)
  return mean, sd

#CURRENTLY NOT USED
def create_distribution(domainlist):
  dist = dict()
  i = 0
  total = 0
  while i < len(alphanumeric):
    dist[alphanumeric[i]] = 0
    i+=1
  for domain in domainlist:
    num = 0
    while num < len(domain):
      char = domain[num]
      curr = dist[char]
      curr+=1
      dist[char]=curr
      total+=1
      num+=1
  i = 0
  while i < len(alphanumeric):
    curr = float(dist[alphanumeric[i]])
    curr = curr/total
    dist[alphanumeric[i]] = curr
    i+=1
  return dist

#CURRENTLY NOT USED
def KLUnigram(test_dist,benign_metric,mal_metric):
  firstsum = 0
  secondsum = 0
  i = 0
  while i <len(alphanumeric):
    char = alphanumeric[i]
    if(benign_metric[char]!=0 and test_dist[char]!=0):
      firstsum+=(test_dist[char]*math.log(test_dist[char]/benign_metric[char]))
      secondsum += (benign_metric[char]*math.log(benign_metric[char]/test_dist[char]))
    i+=1
  non_malicious_d = 0.5*(firstsum+secondsum)
  firstsum = 0
  secondsum=0
  i=0
  while i <len(alphanumeric):
    char = alphanumeric[i]
    if(mal_metric[char]!=0 and test_dist[char]!=0):
      firstsum+=(test_dist[char]*math.log(test_dist[char]/mal_metric[char]))
      secondsum += (mal_metric[char]*math.log(mal_metric[char]/test_dist[char]))
    i+=1
  malicious_d = 0.5*(firstsum+secondsum)
  if(non_malicious_d<malicious_d):
    return 1
  else:
    return -1
    
def EditDistance(s1, s2):
  m = []
  i = 0
  while i < len(s1)+1:
    m.append([])
    n = 0
    while n <len(s2)+1:
      m[i].append(0)
      n+=1
    i+=1
  i = 0
  while i < len(s1)+1:
    m[i][0]=i
    i+=1
  j = 0
  while j < len(s2)+1:
    m[0][j] = j
    j+=1
  i = 1
  while i < len(s1)+1:
    j = 1
    while j < len(s2)+1:
      rand = 0 
      if(s1[i-1]!=s2[j-1]):
	rand = 1
      m[i][j] = min(min(m[i-1][j-1]+rand,m[i-1][j]+1),m[i][j-1]+1)
      j+=1
    i+=1
  return m[len(s1)][len(s2)]

#Returns the Number of A records, Number of distinct ASNs, TTL, and Number of Distinct Prefixes
def num_A_records(domain):
  newdomain = domain
  if newdomain not in lookups:
    lookups[newdomain] = []
  #Perform a dig lookup of the domain and write the output to a file
  process = subprocess.Popen('dig "%s">output.txt'%newdomain, shell=True)
  process.wait()
  result = open('output.txt','r')
  s = []
  for line in result:
    s.append(line)
  result.close()
  n = 0
  listASN = []
  listPrefix = []
  numIP = 0
  while n<len(s):
    if s[n].rstrip() == ";; ANSWER SECTION:": 	#FIND ANSWER SECTION
      n+=1
      break
    n+=1
  TTL = 0
  while n<len(s):
    if (s[n] != '\n'):
      IP = s[n].rstrip()			#One A Record
      numIP+=1
      words = str.split(IP)
      IP = words[len(words)-1]			#Get IP address of A record
      TTL = int(words[1])			#Get TTL
      if newdomain in lookups:			#Append IP address to the list for that domain
	curr = lookups[newdomain]
	if(IP not in curr):
	  curr.append(IP)
	lookups[newdomain] = curr
      else:
	lookups[newdomain] = [IP]
      #Perform a whois lookup of the current IP address and write output to file
      whois = subprocess.Popen('whois -h whois.cymru.com " -v %s">whoisoutput.txt'%IP, shell=True)
      whois.wait()
      whoisresult = open('whoisoutput.txt','r')
      news = []
      for line in whoisresult:
	news.append(line)
      whoisresult.close()
      #Find the ASN and BGP prefix of the whois lookup. If not already in the list (distinct), then add them to list
      if len(news)>1:
	ASN = news[1].rstrip()
	charac = str.split(ASN)
	ASN = charac[0]
	if ASN not in listASN:
	  listASN.append(ASN)
	BGP = charac[2]
	if BGP not in listPrefix:
	  listPrefix.append(BGP)
    else:
      break
    n+=1
  return numIP, len(listASN), TTL, len(listPrefix)

#Returns Number of nameserver records for a domain
def num_NS(domain):
  newdomain = domain
  if newdomain not in NSlookups:
    NSlookups[newdomain] = []
  process = subprocess.Popen('dig %s NS>output.txt'%newdomain, shell=True)
  process.wait()
  result = open('output.txt','r')
  s = []
  for line in result:
      s.append(line)
  result.close()
  numNS = 0
  n=0
  while n<len(s):
    if s[n].rstrip() == ";; ANSWER SECTION:":
      n+=1
      break
    n+=1
  while n<len(s):
    if (s[n] != '\n'):
      numNS+=1
      NS = s[n].rstrip()
      words = str.split(NS)
      NS = words[len(words)-1]
      #Peform a dig lookup of the nameserver, in order to get the IP addresses
      dig = subprocess.Popen('dig %s>NSoutput.txt'%NS,shell=True)
      dig.wait()
      NSresult = open('NSoutput.txt','r')
      news = []
      for line in NSresult:
	news.append(line)
      NSresult.close()
      i = 0
      while i<len(news):
	if news[i].rstrip() == ";; ANSWER SECTION:":
	  i+=1
	  break
	i+=1
      while i<len(news):
	if(news[i] != '\n'):
	  IP = news[i].rstrip()
	  words = str.split(IP)
	  IP = words[len(words)-1]
	  if newdomain in NSlookups:		#Makes a list of all the IP addreses for all the nameservers of a domain
	    curr = NSlookups[newdomain]
	    if(IP not in curr):
	      curr.append(IP)
	    NSlookups[newdomain] = curr
	  else:
	    NSlookups[newdomain] = [IP]
	i+=1
    else:
      break
    n+=1
  return numNS

#Gets the locations (lat, long) and timezones of each IP address for each A record and nameserver
def get_locations(domain):
    IPs = lookups[domain]		#list of IP addresses for a particular domain
    geoA[domain] = []			#latitude,longitude
    timezonesA[domain] = []		#time zone
    for IP in IPs:
      #queries an external database to retrieve information about the IP address
      loc = subprocess.Popen('wget "http://api.ipinfodb.com/v3/ip-city/?key=8fcf77ba809cd87fc56bd048ee5c8d31a14fd52fa97431525f14dfcf334ccea5&timezone=true&format=json&ip=%s" -O ipgeo.txt'%IP,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
      loc.wait()
      location = open('ipgeo.txt','r')
      lines = location.readlines()
      location.close()
      coords = []		#[latitude,longitude]
      timezone = ''
      for line in lines:
	#gets latitude
	if line.find('latitude') >= 0:
	  parts = line.split()
	  if(len(parts)>1):
	    content = parts[2]
	    first = content.find('\"')
	    last = content.rfind('\"')
	    if len(content[first+1:last])>1:
	      coords.append(content[first+1:last])
	#gets longitude
	if line.find('longitude')>=0:
	  parts = line.split()
	  if(len(parts)>1):
	    content = parts[2]
	    first = content.find('\"')
	    last = content.rfind('\"')
	    if len(content[first+1:last])>1:
	      coords.append(content[first+1:last])
	#gets Time Zone
	if line.find('timeZone')>=0:
	  parts = line.split()
	  if(len(parts)>1):
	    content = parts[2]
	    first = content.find('\"')
	    last = content.rfind(':')
	    if len(content[first+1:last])>1:
	      timezone = content[first+1:last]
      if(len(coords)>0):
	curr = geoA[domain]
	curr.append(coords)
	geoA[domain] = curr		#appends coordinates of that IP address to the dictionary for the domain	
      if(len(timezone)>0):
	curr = timezonesA[domain]
	curr.append(timezone)
	timezonesA[domain] = curr	#appends time zone of that IP address to the dictionary for the domain
    print 'Done with A record locations'
    #Same as above, except with the NS records
    IPs = NSlookups[domain]		#IP addresses of nameserver, follows same process as for A record
    geoNS[domain] = []
    timezonesNS[domain] = []
    for IP in IPs:
      loc = subprocess.Popen('wget "http://api.ipinfodb.com/v3/ip-city/?key=8fcf77ba809cd87fc56bd048ee5c8d31a14fd52fa97431525f14dfcf334ccea5&timezone=true&format=json&ip=%s" -O ipgeo.txt'%IP,shell=True,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
      loc.wait()
      location = open('ipgeo.txt','r')
      lines = location.readlines()
      location.close()
      coords = []
      timezone = ''
      for line in lines:
	if line.find('latitude') >= 0:
	  parts = line.split()
	  if(len(parts)>1):
	    content = parts[2]
	    first = content.find('\"')
	    last = content.rfind('\"')
	    if len(content[first+1:last])>1:
	      coords.append(content[first+1:last])
	if line.find('longitude')>=0:
	  parts = line.split()
	  if(len(parts)>1):
	    content = parts[2]
	    first = content.find('\"')
	    last = content.rfind('\"')
	    if len(content[first+1:last])>1:
	      coords.append(content[first+1:last])
	if line.find('timeZone')>=0:
	  parts = line.split()
	  if(len(parts)>1):
	    content = parts[2]
	    first = content.find('\"')
	    last = content.rfind(':')
	    if len(content[first+1:last])>1:
	      timezone = content[first+1:last]
      if len(coords)>0:
	curr = geoNS[domain]
	curr.append(coords)
	geoNS[domain] = curr
      if len(timezone)>0:
	curr = timezonesNS[domain]
	curr.append(timezone)
	timezonesNS[domain] = curr

#Get the Spatial Uniform Distribution of A records and NS records (for formula see documentation)
def get_entropy(domain):
  A_entropy = 0
  NS_entropy=0
  #list of all GMT time zones
  zones  = ['-11','-10','-09','-08','-07','-06','-05','-04','-03','-02','-01','+00','+01','+02','+03','+04','+05','+06','+07','+08','+09','+10','+11','+12']
  if(len(lookups[domain])>0):
    for z in zones:
      num = 0
      for timezone in timezonesA[domain]:
	if timezone == z:
	  num+=1
      summation = 0
      if num!=0:
	div = float(num*1.0/len(lookups[domain]))
	summation = div * math.log(div,10)
      A_entropy += summation
  if(len(NSlookups[domain])>0):
    for z in zones:
      num = 0
      for timezone in timezonesNS[domain]:
	if timezone == z:
	  num+=1
      summation = 0
      if num!=0:
	div = float(num*1.0/len(NSlookups[domain]))
	summation = div * math.log(div,10)
      NS_entropy += summation
  return -A_entropy, -NS_entropy

#returns average Minimal Service Distance and Standard Deviation (for formula see Documentation)
def get_MSD(domain):
  IPs = geoA[domain]		#list of coordinates of all IP addreses for a particular domain's A record
  NSs = geoNS[domain]		#list of coordinates of all IP addreses for a particular domain's NS record
  ave = 0
  sd = 0
  i = 0
  mins = []
  if(len(IPs)<=0):
    mins.append(0)
  for loc in IPs:
    arr = []
    lat = float(loc[0])
    lon = float(loc[1])
    for NS in NSs:
      nlat = float(NS[0])
      nlon = float(NS[1])
      val = math.sqrt(math.pow((lat-nlat),2)+math.pow((lon-nlon),2))
      arr.append(val)
    if len(arr)>0:
      mins.append(min(arr))
    else:
      mins.append(0)
    i+=1
  ave, sd = standev(mins)
  return ave, sd
    
def write_A_lookups():
  writer = open('A_lookups.txt','w')
  for domain in lookups.keys():
    writer.write('%s\n'%domain)
    for IP in lookups[domain]:
      writer.write('%s\n'%IP)
    writer.write('\n')
  writer.flush()
  writer.close()
  
def write_NS_lookups():
  writer = open('NS_lookups.txt','w')
  for domain in NSlookups.keys():
    writer.write('%s\n'%domain)
    for IP in NSlookups[domain]:
      writer.write('%s\n'%IP)
    writer.write('\n')
  writer.flush()
  writer.close()
  
def write_A_locations():
  writer = open('A_locations.txt','w')
  for domain in geoA.keys():
    writer.write('%s\n'%domain)
    for coords in geoA[domain]:
      writer.write('%s,%s\n'%(coords[0],coords[1]))
    writer.write('\n')
  writer.flush()
  writer.close()
  
def write_NS_locations():
  writer = open('NS_locations.txt','w')
  for domain in geoNS.keys():
    writer.write('%s\n'%domain)
    for coords in geoNS[domain]:
      writer.write('%s,%s\n'%(coords[0],coords[1]))
    writer.write('\n')
  writer.flush()
  writer.close()
  
def write_A_timezones():
  writer = open('A_timezones.txt','w')
  for domain in timezonesA.keys():
    writer.write('%s\n'%domain)
    for timezone in timezonesA[domain]:
      writer.write('%s\n'%timezone)
    writer.write('\n')
  writer.flush()
  writer.close()
  
def write_NS_timezones():
  writer = open('NS_timezones.txt','w')
  for domain in timezonesNS.keys():
    writer.write('%s\n'%domain)
    for timezone in timezonesNS[domain]:
      writer.write('%s\n'%timezone)
    writer.write('\n')
  writer.flush()
  writer.close()

def read_lookups():
  reader = open('A_lookups.txt','r')
  lines = reader.readlines()
  reader.close()
  i = 0
  while i < len(lines):
    line = lines[i].rstrip()
    lookups[line] = []
    i+=1
    while i<len(lines):
      IP = lines[i]
      if (IP == '\n'):
	i+=1
	break
      curr = lookups[line]
      curr.append(IP.rstrip())
      lookups[line] = curr
      i+=1
      
def read_NSlookups():
  reader = open('NS_lookups.txt','r')
  lines = reader.readlines()
  reader.close()
  i = 0
  while i < len(lines):
    line = lines[i].rstrip()
    NSlookups[line] = []
    i+=1
    while i<len(lines):
      IP = lines[i]
      if (IP == '\n'):
	i+=1
	break
      curr = NSlookups[line]
      curr.append(IP.rstrip())
      NSlookups[line] = curr
      i+=1
      
def read_A_locations():
  reader = open('A_locations.txt','r')
  lines = reader.readlines()
  reader.close()
  i = 0
  while i < len(lines):
    line = lines[i].rstrip()
    geoA[line] = []
    i+=1
    while i<len(lines):
      loc = lines[i]
      if (loc == '\n'):
	i+=1
	break
      loc=loc.rstrip()
      coords = loc.split(',')
      curr = geoA[line]
      curr.append(coords)
      geoA[line] = curr
      i+=1
      
def read_NS_locations():
  reader = open('NS_locations.txt','r')
  lines = reader.readlines()
  reader.close()
  i = 0
  while i < len(lines):
    line = lines[i].rstrip()
    geoNS[line] = []
    i+=1
    while i<len(lines):
      loc = lines[i]
      if (loc == '\n'):
	i+=1
	break
      loc = loc.rstrip()
      coords = loc.split(',')
      curr = geoNS[line]
      curr.append(coords)
      geoNS[line] = curr
      i+=1
      
def read_A_timezones():
  reader = open('A_timezones.txt','r')
  lines = reader.readlines()
  reader.close()
  i = 0
  while i < len(lines):
    line = lines[i].rstrip()
    timezonesA[line] = []
    i+=1
    while i<len(lines):
      tz = lines[i]
      if (tz == '\n'):
	i+=1
	break
      curr = timezonesA[line]
      curr.append(tz.rstrip())
      timezonesA[line] = curr
      i+=1
      
def read_NS_timezones():
  reader = open('NS_timezones.txt','r')
  lines = reader.readlines()
  reader.close()
  i = 0
  while i < len(lines):
    line = lines[i].rstrip()
    timezonesNS[line] = []
    i+=1
    while i<len(lines):
      tz = lines[i]
      if (tz == '\n'):
	i+=1
	break
      curr = timezonesNS[line]
      curr.append(tz.rstrip())
      timezonesNS[line] = curr
      i+=1

#NOT CURRENTLY USED
def create_shortened_domain_list(domains,domaintype):
  for domain in domains:
    short = domain
    short = short.replace('.','')
    short = short.replace('-','')
    domaintype.append(short)
    
def read_delays():
  ndfile = open('network_delays_benign.txt','r')
  dfdfile = open('document_fetch_delays_benign.txt','r')
  pdfile = open('processing_delays_benign.txt','r')
  ndlist = ndfile.readlines()
  ndfile.close()
  dfdlist = dfdfile.readlines()
  dfdfile.close()
  pdlist = pdfile.readlines()
  pdfile.close()
  i = 0
  while i < len(ndlist):
    line = ndlist[i]
    parts = line.rstrip().split()
    nd[parts[0]] = [float(parts[1]),float(parts[2])]
    line = dfdlist[i]
    parts = line.rstrip().split()
    dfd[parts[0]] = [float(parts[1]),float(parts[2])]
    line = pdlist[i]
    parts = line.rstrip().split()
    pd[parts[0]] = [float(parts[1]),float(parts[2])]
    i+=1
  ndfile = open('network_delays_fastflux.txt','r')
  dfdfile = open('document_fetch_delays_fastflux.txt','r')
  pdfile = open('processing_delays_fastflux.txt','r')
  ndlist = ndfile.readlines()
  ndfile.close()
  dfdlist = dfdfile.readlines()
  dfdfile.close()
  pdlist = pdfile.readlines()
  pdfile.close()
  i = 0
  while i < len(ndlist):
    line = ndlist[i]
    parts = line.rstrip().split()
    nd[parts[0]] = [float(parts[1]),float(parts[2])]
    line = dfdlist[i]
    parts = line.rstrip().split()
    dfd[parts[0]] = [float(parts[1]),float(parts[2])]
    line = pdlist[i]
    parts = line.rstrip().split()
    pd[parts[0]] = [float(parts[1]),float(parts[2])]
    i+=1

def write_delays():
  ndwriter = open('network_delays_benign.txt','w')
  dfdwriter = open('document_fetch_delays_benign.txt','w')
  pdwriter = open('processing_delays_benign.txt','w')
  for line in benign:
    ndwriter.write('%s\t%f\t%f\n'%(line,nd[line][0],nd[line][1]))
    dfdwriter.write('%s\t%f\t%f\n'%(line,dfd[line][0],dfd[line][1]))
    pdwriter.write('%s\t%f\t%f\n'%(line,pd[line][0],pd[line][1]))
  ndwriter.flush()
  dfdwriter.flush()
  pdwriter.flush()
  ndwriter.close()
  dfdwriter.close()
  pdwriter.close()
  ndwriter = open('network_delays_fastflux.txt','w')
  dfdwriter = open('document_fetch_delays_fastflux.txt','w')
  pdwriter = open('processing_delays_fastflux.txt','w')
  for line in malicious:
    ndwriter.write('%s\t%f\t%f\n'%(line,nd[line][0],nd[line][1]))
    dfdwriter.write('%s\t%f\t%f\n'%(line,dfd[line][0],dfd[line][1]))
    pdwriter.write('%s\t%f\t%f\n'%(line,pd[line][0],pd[line][1]))
  ndwriter.flush()
  dfdwriter.flush()
  pdwriter.flush()
  ndwriter.close()
  dfdwriter.close()
  pdwriter.close()	

#Finds Heuristics for the entire training set and writes to the text file
def create_heuristics():
  trainfile = open('data.txt','w')
  benigndomains = []
  maliciousdomains = []
  #create_shortened_domain_list(benign,benigndomains)
  #create_shortened_domain_list(malicious,maliciousdomains)
  #All of the benign URLs
  for url in benign:
    print 'Currently doing %s...'%url
    domain = url
    #Finds the Domain Name of a URL
    netloc = urlparse(domain)[1]
    if(netloc != ''):
      if (is_valid_ip(netloc) is False):
	domain = getDomain(domain, tlds)
      else:
	domain = netloc
    numA, numASN, TTL, num_distinct_networks = num_A_records(domain)
    numNS = num_NS(domain)
    print 'Done with Lookups'
    #Need to Re-Calculate Certain Heuristics
    if(response != 'y'):
      get_locations(domain)
      if (len(lookups[domain])>0):
	delays = get_delays('%s'%url)
	networkdelay= delays[0]
	documentfetchdelay= delays[1]
	processingdelay= delays[2]
	ndave, sdND = standev(networkdelay)
	dfdave, sdDFD = standev(documentfetchdelay)
	pdave, sdPD = standev(processingdelay)
	nd[url] = [ndave,sdND]				#append to dictionary for that url
	dfd[url] = [dfdave, sdDFD]
	pd[url] = [pdave,sdPD]
      else:
	nd[url] = [0,0]					#if the client cannot resolve the hostname, then by default the values are 0
	dfd[url] = [0,0]
	pd[url] = [0,0]
    a_entropy, ns_entropy=get_entropy(url)		#always calculated
    MSD_ave, MSD_sd = get_MSD(url)			#always calculated
    heuristics[url] = [numA, numASN, TTL, num_distinct_networks,numNS, a_entropy, ns_entropy, MSD_ave, MSD_sd, nd[url][0], nd[url][1], dfd[url][0],dfd[url][1] , pd[url][0], pd[url][1]]
    print 'Done with %s'%url
  #Same process as above, except for malicious URLs
  for url in malicious:
    print 'Currently doing %s...'%url
    domain = url
    netloc = urlparse(domain)[1]
    if(netloc != ''):
      if (is_valid_ip(netloc) is False):
	domain = getDomain(domain, tlds)
      else:
	domain = netloc
    numA, numASN, TTL, num_distinct_networks = num_A_records(domain)
    numNS = num_NS(domain)
    print 'Done with Lookups'
    if(response != 'y'):
      get_locations(domain)
      if (len(lookups[domain])>0):
	delays = get_delays('%s'%url)
	networkdelay= delays[0]
	documentfetchdelay= delays[1]
	processingdelay= delays[2]
	ndave, sdND = standev(networkdelay)
	dfdave, sdDFD = standev(documentfetchdelay)
	pdave, sdPD = standev(processingdelay)
	nd[url] = [ndave,sdND]
	dfd[url] = [dfdave, sdDFD]
	pd[url] = [pdave,sdPD]
      else:
	nd[url] = [0,0]
	dfd[url] = [0,0]
	pd[url] = [0,0]
    a_entropy, ns_entropy=get_entropy(url)
    MSD_ave, MSD_sd = get_MSD(url)
    heuristics[url] = [numA, numASN, TTL, num_distinct_networks,numNS, a_entropy, ns_entropy, MSD_ave, MSD_sd, nd[url][0], nd[url][1], dfd[url][0],dfd[url][1] , pd[url][0], pd[url][1]]
    print 'Done with %s'%url
  #Write Heuristics to arff File, along with 1 for benign
  for url in benign:
    arr = heuristics[url]
    for heur in arr:
      trainfile.write('%f,'%heur)
    trainfile.write('1\n')
  #Write Heuristics to arff File, along with -1 for fastflux
  for url in malicious:
    arr = heuristics[url]
    for heur in arr:
      trainfile.write('%f,'%heur)
    trainfile.write('-1\n')
  #write certain dictionaries to respective files
  write_A_lookups()
  write_NS_lookups()
  write_A_locations()
  write_NS_locations()
  write_A_timezones()
  write_NS_timezones()
  write_delays()

#MAIN
if __name__ == "__main__":
  #Get Top-Level Domains
  with open("effective_tld_names.dat.txt") as tldFile:
    tlds = set([line.strip() for line in tldFile if line[0] not in "/\n"]) 
  #These Dictionaries are used for Storing Various Pieces of Data for each URL/Domain
  lookups = dict()
  NSlookups = dict()
  geoA = dict()
  geoNS = dict()
  timezonesA = dict()
  timezonesNS = dict()
  nd = dict()
  dfd = dict()
  pd = dict()
  heuristics = dict()
  '''
  
  THE FOLLOWING SECTION OF CODE IS USED TO TRAIN THE CLASSIFIER OR TEST PRE-LABELED URLs. FOR FURTHER INSTRUCTIONS, SEE THE README FILE
  
  '''
  benign = []
  malicious = []
  train = raw_input('Train or Test Pre-Labeled URLs? (1 for Train, 2 for Test)')
  if(train == '1'):
    read_file('benign.txt',benign)
    read_file('malicious.txt',malicious)
  else:
    read_file('testbenign.txt',benign)
    read_file('testmalicious.txt',malicious)
  response = raw_input('Read from files [y/n]? ')
  if response == 'y':
    read_lookups()
    read_NSlookups()
    read_A_locations()
    read_NS_locations()
    read_A_timezones()
    read_NS_timezones() 
    read_delays()
  create_heuristics()
  '''
  
  THE FOLLOWING SECTION OF CODE IS USED TO CLASSIFY TEST UN-LABELED URLs. FOR MORE INSTRUCTIONS, SEE THE README FILE
  '''
  '''
  testurls = []
  testfile = open('test.txt','r')
  domains = testfile.readlines()
  testfile.close()
  for domain in domains:
    testurls.append(domain.rstrip())
  #Arff file
  trainfile = open('numerictesting.arff','w')
  trainfile.write('@relation numerictesting\n\n')
  trainfile.write('@attribute num_A numeric\n')
  trainfile.write('@attribute num_ASN numeric\n')
  trainfile.write('@attribute TTL numeric\n')
  trainfile.write('@attribute num_distinct_networks numeric\n')
  trainfile.write('@attribute num_NS numeric\n')
  trainfile.write('@attribute entropy_A numeric\n')
  trainfile.write('@attribute entropy_NS numeric\n')
  trainfile.write('@attribute ave_MSD numeric\n')
  trainfile.write('@attribute sd_MSD numeric\n')
  trainfile.write('@attribute ND numeric\n')
  trainfile.write('@attribute sd_ND numeric\n')
  trainfile.write('@attribute DFD numeric\n')
  trainfile.write('@attribute sd_DFD numeric\n')
  trainfile.write('@attribute PD numeric\n')
  trainfile.write('@attribute sd_PD numeric\n')
  trainfile.write('@attribute fastflux {-1,1}\n\n')
  trainfile.write('@data\n')
  #Calculate Heuristics for each URL
  for url in testurls:
    print 'Currently doing %s...'%url
    domain = url
    netloc = urlparse(domain)[1]
    if(netloc != ''):
      if (is_valid_ip(netloc) is False):
	domain = getDomain(domain, tlds)
      else:
	domain = netloc
    numA, numASN, TTL, num_distinct_networks = num_A_records(domain)
    numNS = num_NS(domain)
    print 'Done with Lookups'
    get_locations(domain)
    if (len(lookups[domain])>0):
	delays = get_delays('%s'%url)
	networkdelay= delays[0]
	documentfetchdelay= delays[1]
	processingdelay= delays[2]
	ndave, sdND = standev(networkdelay)
	dfdave, sdDFD = standev(documentfetchdelay)
	pdave, sdPD = standev(processingdelay)
	nd[url] = [ndave,sdND]
	dfd[url] = [dfdave, sdDFD]
	pd[url] = [pdave,sdPD]
    else:
	nd[url] = [0,0]
	dfd[url] = [0,0]
	pd[url] = [0,0]
    a_entropy, ns_entropy=get_entropy(url)
    MSD_ave, MSD_sd = get_MSD(url)
    heuristics[url] = [numA, numASN, TTL, num_distinct_networks,numNS, a_entropy, ns_entropy, MSD_ave, MSD_sd, nd[url][0], nd[url][1], dfd[url][0],dfd[url][1] , pd[url][0], pd[url][1]]
    print 'Done with %s'%url
  #Write Heuristics to ARFF file
  for url in testurls:
    arr = heuristics[url]
    for heur in arr:
      trainfile.write('%f,'%heur)
    trainfile.write('-1\n')
  trainfile.flush()
  trainfile.close()
  #JAVA Virtual Machine
  if not jpype.isJVMStarted():
	_jvmArgs = ["-ea"] # enable assertions
	_jvmArgs.append("-Djava.class.path="+os.environ['CLASSPATH'])
	jpype.startJVM(jpype.getDefaultJVMPath(), *_jvmArgs)
  #program can now recognize the weka API
  weka = jpype.JPackage("weka")

  JPypeObjectInputStream = jpype.JClass("JPypeObjectInputStream")
  #Custom Class
  class WekaClassifier(object):
	def __init__(self, modelFilename, datasetFilename):
		self.dataset = weka.core.Instances(
			java.io.FileReader(datasetFilename))		
		self.dataset.setClassIndex(self.dataset.numAttributes() - 1)
		ois = JPypeObjectInputStream(
			java.io.FileInputStream(modelFilename))
		self.model = ois.readObject()
		ois.close()
  model = raw_input('Enter exact model filename: ')
  classifier = WekaClassifier(model, 'numerictesting.arff')
  i = 0
  #writes results (1 for benign, 0 for fastflux) to file
  resultswriter = open('testresults.txt','w')
  while i<classifier.dataset.numInstances():
    label = classifier.model.classifyInstance(classifier.dataset.instance(i))
    resultswriter.write('%s\n'%str(label))
    i+=1
  resultswriter.flush()
  resultswriter.close()
  jpype.shutdownJVM()
  '''